√Ångel David 
Eduardo Collado
Roberto Rom√°n

from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, learning_curve
from sklearn.preprocessing import StandardScaler
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.svm import SVC
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import (accuracy_score, classification_report, confusion_matrix,
                            roc_curve, auc, roc_auc_score)
####################################################################################
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
import warnings
warnings.filterwarnings('ignore')

plt.style.use('seaborn-v0_8-darkgrid')
sns.set_palette("husl")

# ==============================================================================
# 1. FUNCI√ìN PARA GRAFICAR LA CURVA DE APRENDIZAJE (NUEVO)
# ==============================================================================
def plot_learning_curve(estimator, title, X, y, axes=None, ylim=None, cv=None,
                        n_jobs=None, train_sizes=np.linspace(.1, 1.0, 5)):
    """Genera 3 plots: la curva de aprendizaje, la curva de tiempo de escalado y la curva de rendimiento vs. tiempo."""
    if axes is None:
        _, axes = plt.subplots(1, 3, figsize=(20, 5))

    axes[0].set_title(title)
    if ylim is not None:
        axes[0].set_ylim(*ylim)
    axes[0].set_xlabel("N√∫mero de ejemplos de entrenamiento")
    axes[0].set_ylabel("Puntuaci√≥n (Score)")

    train_sizes, train_scores, test_scores, fit_times, _ = \
        learning_curve(estimator, X, y, cv=cv, n_jobs=n_jobs,
                       train_sizes=train_sizes,
                       scoring='accuracy', # Usamos la precisi√≥n como m√©trica
                       return_times=True)
    
    train_scores_mean = np.mean(train_scores, axis=1)
    train_scores_std = np.std(train_scores, axis=1)
    test_scores_mean = np.mean(test_scores, axis=1)
    test_scores_std = np.std(test_scores, axis=1)
    fit_times_mean = np.mean(fit_times, axis=1)
    fit_times_std = np.std(fit_times, axis=1)

    # Plot de la Curva de Aprendizaje
    axes[0].grid()
    axes[0].fill_between(train_sizes, train_scores_mean - train_scores_std,
                         train_scores_mean + train_scores_std, alpha=0.1,
                         color="r")
    axes[0].fill_between(train_sizes, test_scores_mean - test_scores_std,
                         test_scores_mean + test_scores_std, alpha=0.1,
                         color="g")
    axes[0].plot(train_sizes, train_scores_mean, 'o-', color="r",
                 label="Puntuaci√≥n de Entrenamiento")
    axes[0].plot(train_sizes, test_scores_mean, 'o-', color="g",
                 label="Puntuaci√≥n de Validaci√≥n Cruzada")
    axes[0].legend(loc="best")
    
    # Plot de Tiempos de Ajuste vs. Tama√±o de Entrenamiento
    axes[1].grid()
    axes[1].plot(train_sizes, fit_times_mean, 'o-')
    axes[1].fill_between(train_sizes, fit_times_mean - fit_times_std,
                         fit_times_mean + fit_times_std, alpha=0.1)
    axes[1].set_xlabel("N√∫mero de ejemplos de entrenamiento")
    axes[1].set_ylabel("Tiempo de ajuste (segundos)")
    axes[1].set_title("Curva de Tiempo de Escalado")

    # Plot de Puntuaci√≥n vs. Tiempo de Ajuste
    axes[2].grid()
    axes[2].plot(fit_times_mean, test_scores_mean, 'o-')
    axes[2].set_xlabel("Tiempo de ajuste (segundos)")
    axes[2].set_ylabel("Puntuaci√≥n de Validaci√≥n Cruzada")
    axes[2].set_title("Rendimiento vs. Tiempo de Entrenamiento")
    
    return plt

# ==============================================================================
# 2. CARGA Y PREPARACI√ìN DE DATOS (SIN CAMBIOS)
# ==============================================================================
print("=" * 80)
print(" " * 20 + "üè• CLASIFICACI√ìN AVANZADA DE C√ÅNCER DE PECHO üè•")
print("=" * 80)

datos = load_breast_cancer()
X = datos.data
y = datos.target

df = pd.DataFrame(X, columns=datos.feature_names)
df['target'] = y

print(f"\nüìä INFORMACI√ìN DEL DATASET")
print(f"   Total de muestras: {X.shape[0]}")
print(f"   Caracter√≠sticas por muestra: {X.shape[1]}")
print(f"   Clases: {list(datos.target_names)}")

casos_malignos = np.sum(y == 0)
casos_benignos = np.sum(y == 1)
print(f"   ‚Ä¢ Casos malignos: {casos_malignos} ({casos_malignos/len(y)*100:.1f}%)")
print(f"   ‚Ä¢ Casos benignos: {casos_benignos} ({casos_benignos/len(y)*100:.1f}%)")
X_train, X_test, y_train, y_test = train_test_split(
    X, y,
    test_size=0.2,
    random_state=42,
    stratify=y
)

scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# ==============================================================================
# 3. ENTRENAMIENTO Y EVALUACI√ìN INICIAL DE MODELOS (SIN CAMBIOS)
# ==============================================================================
modelos = {
    'KNN': KNeighborsClassifier(n_neighbors=5),
    'Random Forest': RandomForestClassifier(n_estimators=200, random_state=42, max_depth=10),
    'SVM': SVC(kernel='rbf', probability=True, random_state=42, C=1.0, gamma='scale'),
    'Logistic Regression': LogisticRegression(max_iter=10000, random_state=42),
    'Gradient Boosting': GradientBoostingClassifier(n_estimators=100, random_state=42, learning_rate=0.1)
}

from sklearn.ensemble import VotingClassifier

voting_model = VotingClassifier(
    estimators=[
        ('knn', KNeighborsClassifier(n_neighbors=5)),
        ('rf', RandomForestClassifier(n_estimators=200, random_state=42, max_depth=10)),
        ('svm', SVC(kernel='rbf', probability=True, random_state=42, C=1.0, gamma='scale')),
        ('logreg', LogisticRegression(max_iter=10000, random_state=42)),
        ('gb', GradientBoostingClassifier(n_estimators=100, random_state=42, learning_rate=0.1)),
    ],
    voting='soft'
)

modelos['Voting Ensemble'] = voting_model

resultados = {}
predicciones = {}
probabilidades = {}

print(f"\n{'=' * 80}")
print("üî¨ ENTRENAMIENTO Y EVALUACI√ìN INICIAL DE MODELOS")
print(f"{'=' * 80}\n")

for nombre, modelo in modelos.items():
    print(f"Entrenando {nombre}...", end=" ")
    
    modelo.fit(X_train_scaled, y_train)
    
    y_pred = modelo.predict(X_test_scaled)
    y_proba = modelo.predict_proba(X_test_scaled)[:, 1]
    
    predicciones[nombre] = y_pred
    probabilidades[nombre] = y_proba
    
    accuracy = accuracy_score(y_test, y_pred)
    # Solo en entrenamiento para CV para evitar fuga de datos
    cv_scores = cross_val_score(modelo, X_train_scaled, y_train, cv=5) 
    roc_auc = roc_auc_score(y_test, y_proba)
    
    resultados[nombre] = {
        'accuracy': accuracy,
        'cv_mean': cv_scores.mean(),
        'cv_std': cv_scores.std(),
        'roc_auc': roc_auc
    }
    
    print(f"‚úì Precisi√≥n: {accuracy:.2%} | CV: {cv_scores.mean():.2%} (¬±{cv_scores.std():.2%}) | AUC: {roc_auc:.3f}")

# Determinar el mejor modelo (usando la precisi√≥n del conjunto de prueba)
mejor_modelo_nombre = max(resultados, key=lambda x: resultados[x]['accuracy'])

print(f"\n{'=' * 80}")
print(f"üèÜ MEJOR MODELO INICIAL: {mejor_modelo_nombre} con Precisi√≥n de {resultados[mejor_modelo_nombre]['accuracy']:.2%}")
print(f"{'=' * 80}\n")

# ==============================================================================
# 4. OPTIMIZACI√ìN DE HIPERPAR√ÅMETROS DEL MEJOR MODELO (NUEVO)
# ==============================================================================
print(f"\n{'=' * 80}")
print(f"‚öôÔ∏è OPTIMIZACI√ìN DE HIPERPAR√ÅMETROS ({mejor_modelo_nombre})")
print(f"{'=' * 80}\n")

# Se elige un modelo que t√≠picamente se beneficia del tuning
if mejor_modelo_nombre == 'Random Forest':
    modelo_a_optimizar = modelos[mejor_modelo_nombre]
    
    # Definici√≥n de la grilla de hiperpar√°metros a buscar
    param_grid = {
        'n_estimators': [50, 100, 200],
        'max_depth': [5, 10, 15, None],
        'min_samples_split': [2, 5, 10],
        'min_samples_leaf': [1, 2, 4],
        'criterion': ['gini', 'entropy']
    }
    
    # Inicializaci√≥n de GridSearchCV
    grid_search = GridSearchCV(
        estimator=modelo_a_optimizar, 
        param_grid=param_grid, 
        cv=5, 
        scoring='accuracy', 
        n_jobs=-1, 
        verbose=1
    )
    
    # Ejecuci√≥n de la b√∫squeda
    print(f"Buscando los mejores hiperpar√°metros para {mejor_modelo_nombre}...")
    grid_search.fit(X_train_scaled, y_train)
    
    # Obtener el modelo optimizado
    mejor_modelo_optimizado = grid_search.best_estimator_
    
    # Re-evaluaci√≥n con el modelo optimizado
    y_pred_opt = mejor_modelo_optimizado.predict(X_test_scaled)
    y_proba_opt = mejor_modelo_optimizado.predict_proba(X_test_scaled)[:, 1]
    accuracy_opt = accuracy_score(y_test, y_pred_opt)
    roc_auc_opt = roc_auc_score(y_test, y_proba_opt)
    
    # Actualizar resultados y modelos
    modelos[f'{mejor_modelo_nombre} (Opt)'] = mejor_modelo_optimizado
    resultados[f'{mejor_modelo_nombre} (Opt)'] = {
        'accuracy': accuracy_opt,
        'cv_mean': grid_search.best_score_, # Mejor puntuaci√≥n de CV
        'cv_std': 0, # No es trivial obtener la desviaci√≥n est√°ndar de la mejor combinaci√≥n
        'roc_auc': roc_auc_opt
    }
    predicciones[f'{mejor_modelo_nombre} (Opt)'] = y_pred_opt
    probabilidades[f'{mejor_modelo_nombre} (Opt)'] = y_proba_opt
    
    print("\n‚úì Optimizaci√≥n Completa.")
    print(f"   Mejores Hiperpar√°metros: {grid_search.best_params_}")
    print(f"   Precisi√≥n Original: {resultados[mejor_modelo_nombre]['accuracy']:.2%}")
    print(f"   Precisi√≥n Optimizada: {accuracy_opt:.2%}")
    print(f"   Mejor CV Score: {grid_search.best_score_:.2%}")
    
    # El mejor modelo final podr√≠a ser el optimizado si es mejor
    if accuracy_opt > resultados[mejor_modelo_nombre]['accuracy']:
        mejor_modelo_nombre = f'{mejor_modelo_nombre} (Opt)'
        print(f"   El modelo optimizado es el nuevo mejor modelo.")
    else:
        print(f"   El modelo original se mantiene como el mejor.")

else:
    print(f"   Optimizaci√≥n de hiperpar√°metros no implementada para {mejor_modelo_nombre} en esta versi√≥n.")

# Actualizar la lista de modelos y resultados despu√©s de la posible optimizaci√≥n
nombres_modelos = list(resultados.keys())

# ==============================================================================
# 5. CURVA DE APRENDIZAJE DEL MEJOR MODELO (NUEVO)
# ==============================================================================
print(f"\n{'=' * 80}")
print(f"üìâ CURVA DE APRENDIZAJE DEL MEJOR MODELO ({mejor_modelo_nombre})")
print(f"{'=' * 80}\n")

plt.figure(figsize=(18, 5))
plot_learning_curve(modelos[mejor_modelo_nombre], 
                    f"Curva de Aprendizaje - {mejor_modelo_nombre}", 
                    X_train_scaled, y_train, cv=5, n_jobs=-1)

plt.tight_layout()
plt.savefig('curva_aprendizaje.png', dpi=300, bbox_inches='tight')
print(f"üíæ Curva de Aprendizaje guardada como 'curva_aprendizaje.png'")
plt.show()


# ==============================================================================
# 6. GR√ÅFICOS DE EVALUACI√ìN COMPLETA (MODIFICADO para incluir modelo optimizado)
# ==============================================================================
print(f"\n{'=' * 80}")
print("üìà RESULTADOS DETALLADOS POR MODELO (incluyendo el optimizado si aplica)")
print(f"{'=' * 80}\n")

for nombre in modelos.keys():
    # Solo mostramos el reporte si el modelo fue entrenado/optimizado
    if nombre in predicciones: 
        print(f"\n{'‚îÄ' * 80}")
        print(f"üìä {nombre}")
        print(f"{'‚îÄ' * 80}")
        print(classification_report(y_test, predicciones[nombre], target_names=datos.target_names))

fig = plt.figure(figsize=(20, 12))
fig.suptitle(f'üè• AN√ÅLISIS COMPLETO DE CLASIFICACI√ìN DE C√ÅNCER DE PECHO (Mejor: {mejor_modelo_nombre}) üè•', 
             fontsize=20, fontweight='bold', y=0.995)

# Gr√°fico 1: Comparaci√≥n de Precisi√≥n (incluye el optimizado)
ax1 = plt.subplot(2, 3, 1)
accuracies = [resultados[m]['accuracy'] for m in nombres_modelos]
bars = ax1.barh(nombres_modelos, accuracies, color=sns.color_palette("husl", len(nombres_modelos)))
ax1.set_xlabel('Precisi√≥n', fontsize=12, fontweight='bold')
ax1.set_title('Comparaci√≥n de Precisi√≥n de Modelos', fontsize=14, fontweight='bold')
ax1.set_xlim([0.9, 1.0])
for i, (bar, acc) in enumerate(zip(bars, accuracies)):
    ax1.text(acc, i, f' {acc:.2%}', va='center', fontweight='bold')
ax1.axvline(x=0.95, color='red', linestyle='--', alpha=0.5, label='95%')
ax1.legend()

# Gr√°fico 2: Curvas ROC (incluye el optimizado)
ax2 = plt.subplot(2, 3, 2)
for nombre in modelos.keys():
    if nombre in probabilidades:
        fpr, tpr, _ = roc_curve(y_test, probabilidades[nombre])
        auc_score = auc(fpr, tpr)
        ax2.plot(fpr, tpr, label=f'{nombre} (AUC={auc_score:.3f})', linewidth=2)
ax2.plot([0, 1], [0, 1], 'k--', label='Azar', linewidth=1)
ax2.set_xlabel('Tasa de Falsos Positivos', fontsize=12, fontweight='bold')
ax2.set_ylabel('Tasa de Verdaderos Positivos', fontsize=12, fontweight='bold')
ax2.set_title('Curvas ROC de Todos los Modelos', fontsize=14, fontweight='bold')
ax2.legend(loc='lower right', fontsize=9)
ax2.grid(True, alpha=0.3)

# Gr√°fico 3: Matriz de Confusi√≥n del Mejor Modelo
ax3 = plt.subplot(2, 3, 3)
cm = confusion_matrix(y_test, predicciones[mejor_modelo_nombre])
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=datos.target_names,
            yticklabels=datos.target_names, cbar_kws={'label': 'Frecuencia'}, ax=ax3)
ax3.set_ylabel('Etiqueta Real', fontsize=12, fontweight='bold')
ax3.set_xlabel('Etiqueta Predicha', fontsize=12, fontweight='bold')
ax3.set_title(f'Matriz de Confusi√≥n - {mejor_modelo_nombre}', fontsize=14, fontweight='bold')

# Gr√°fico 4: Validaci√≥n Cruzada (incluye el optimizado si tiene score)
ax4 = plt.subplot(2, 3, 4)
# Asegurar que solo se grafican los que tienen datos v√°lidos (cv_mean)
cv_data = [(m, resultados[m]['cv_mean'], resultados[m]['cv_std']) for m in nombres_modelos if resultados[m].get('cv_mean') is not None]
cv_nombres = [item[0] for item in cv_data]
cv_means = [item[1] for item in cv_data]
cv_stds = [item[2] for item in cv_data]

if cv_nombres: # Solo graficar si hay datos de CV
    ax4.barh(cv_nombres, cv_means, xerr=cv_stds, 
             color=sns.color_palette("husl", len(cv_nombres)),
             capsize=5, alpha=0.8)
    ax4.set_xlabel('Precisi√≥n (Validaci√≥n Cruzada)', fontsize=12, fontweight='bold')
    ax4.set_title('Validaci√≥n Cruzada (5-Fold) con Desviaci√≥n Est√°ndar', fontsize=14, fontweight='bold')
    ax4.set_xlim([0.9, 1.0])
    for i, (mean, std) in enumerate(zip(cv_means, cv_stds)):
        ax4.text(mean, i, f' {mean:.2%}¬±{std:.2%}', va='center', fontsize=9, fontweight='bold')
else:
    ax4.text(0.5, 0.5, 'Datos de CV no disponibles', ha='center', va='center', transform=ax4.transAxes)
    ax4.set_title('Validaci√≥n Cruzada (5-Fold)', fontsize=14, fontweight='bold')


# Gr√°fico 5: Importancia de Caracter√≠sticas del Mejor Modelo
mejor_modelo = modelos[mejor_modelo_nombre]
if hasattr(mejor_modelo, 'feature_importances_'):
    ax5 = plt.subplot(2, 3, 5)
    importancias = mejor_modelo.feature_importances_
    indices = np.argsort(importancias)[-10:]
    ax5.barh(range(10), importancias[indices], color=sns.color_palette("rocket", 10))
    ax5.set_yticks(range(10))
    ax5.set_yticklabels([datos.feature_names[i] for i in indices], fontsize=9)
    ax5.set_xlabel('Importancia', fontsize=12, fontweight='bold')
    ax5.set_title(f'Top 10 Caracter√≠sticas - {mejor_modelo_nombre}', fontsize=14, fontweight='bold')
elif hasattr(mejor_modelo, 'coef_'):
    ax5 = plt.subplot(2, 3, 5)
    coeficientes = np.abs(mejor_modelo.coef_[0])
    indices = np.argsort(coeficientes)[-10:]
    ax5.barh(range(10), coeficientes[indices], color=sns.color_palette("rocket", 10))
    ax5.set_yticks(range(10))
    ax5.set_yticklabels([datos.feature_names[i] for i in indices], fontsize=9)
    ax5.set_xlabel('Magnitud del Coeficiente', fontsize=12, fontweight='bold')
    ax5.set_title(f'Top 10 Coeficientes - {mejor_modelo_nombre}', fontsize=14, fontweight='bold')
else:
    ax5 = plt.subplot(2, 3, 5)
    ax5.text(0.5, 0.5, f'{mejor_modelo_nombre}\nno proporciona\nimportancia de caracter√≠sticas',
             ha='center', va='center', fontsize=12, transform=ax5.transAxes)
    ax5.set_title(f'Importancia de Caracter√≠sticas - {mejor_modelo_nombre}', fontsize=14, fontweight='bold')
    ax5.axis('off')

# Gr√°fico 6: Distribuci√≥n de Clases (Sin cambios)
ax6 = plt.subplot(2, 3, 6)
clases_counts = [casos_malignos, casos_benignos]
colores = ['#FF6B6B', '#4ECDC4']
wedges, texts, autotexts = ax6.pie(clases_counts, labels=datos.target_names, autopct='%1.1f%%',
                                     colors=colores, startangle=90, textprops={'fontsize': 12, 'fontweight': 'bold'})
ax6.set_title('Distribuci√≥n de Clases en el Dataset', fontsize=14, fontweight='bold')

plt.tight_layout()
plt.savefig('analisis_cancer_completo.png', dpi=300, bbox_inches='tight')
print(f"\nüíæ Gr√°fico guardado como 'analisis_cancer_completo.png'")
plt.show()

# ==============================================================================
# 7. AN√ÅLISIS EXPLORATORIO DE DATOS (Sin cambios)
# ==============================================================================
fig2, axes = plt.subplots(2, 2, figsize=(16, 12))
fig2.suptitle('üìä AN√ÅLISIS EXPLORATORIO DE DATOS', fontsize=18, fontweight='bold')

caracteristicas_importantes = ['mean radius', 'mean texture', 'mean perimeter', 'mean area']

for idx, caracteristica in enumerate(caracteristicas_importantes):
    ax = axes[idx // 2, idx % 2]
    for clase in [0, 1]:
        datos_clase = df[df['target'] == clase][caracteristica]
        ax.hist(datos_clase, bins=30, alpha=0.6, label=datos.target_names[clase], edgecolor='black')
    ax.set_xlabel(caracteristica.title(), fontsize=11, fontweight='bold')
    ax.set_ylabel('Frecuencia', fontsize=11, fontweight='bold')
    ax.set_title(f'Distribuci√≥n: {caracteristica.title()}', fontsize=12, fontweight='bold')
    ax.legend()
    ax.grid(True, alpha=0.3)

plt.tight_layout()
plt.savefig('distribucion_caracteristicas.png', dpi=300, bbox_inches='tight')
print(f"üíæ Gr√°fico guardado como 'distribucion_caracteristicas.png'")
plt.show()

# ==============================================================================
# 8. PREDICCI√ìN CON CASO NUEVO (MODIFICADO para incluir el modelo optimizado)
# ==============================================================================
print(f"\n{'=' * 80}")
print("üéØ PREDICCI√ìN CON CASO NUEVO")
print(f"{'=' * 80}\n")

caso_ejemplo = X_test[0].reshape(1, -1)
caso_escalado = scaler.transform(caso_ejemplo)
etiqueta_real = datos.target_names[y_test[0]]

print(f"Caracter√≠sticas del caso (primeras 5):")
for i in range(5):
    print(f"  ‚Ä¢ {datos.feature_names[i]}: {caso_ejemplo[0][i]:.2f}")

print(f"\nüî¨ Diagn√≥stico Real: {etiqueta_real.upper()}\n")

print(f"{'Modelo':<25} {'Predicci√≥n':<15} {'Confianza':<15} {'Estado'}")
print(f"{'-' * 70}")

for nombre, modelo in modelos.items():
    if nombre in predicciones: # Solo si el modelo fue entrenado/optimizado
        pred = modelo.predict(caso_escalado)[0]
        pred_nombre = datos.target_names[pred]
        proba = modelo.predict_proba(caso_escalado)[0]
        confianza = proba[pred]
        correcto = "‚úì" if pred == y_test[0] else "‚úó"
        print(f"{nombre:<25} {pred_nombre:<15} {confianza:<14.1%} {correcto}")

# ==============================================================================
# 9. RESUMEN FINAL (MODIFICADO para incluir el modelo optimizado)
# ==============================================================================
print(f"\n{'=' * 80}")
print("üèÜ RESUMEN FINAL")
print(f"{'=' * 80}\n")

print(f"‚ú® Mejor Modelo Global: {mejor_modelo_nombre}")
print(f"   ‚Ä¢ Precisi√≥n en Test: {resultados[mejor_modelo_nombre]['accuracy']:.2%}")
print(f"   ‚Ä¢ Validaci√≥n Cruzada: {resultados[mejor_modelo_nombre]['cv_mean']:.2%} (¬±{resultados[mejor_modelo_nombre]['cv_std']:.2%})")
print(f"   ‚Ä¢ AUC-ROC: {resultados[mejor_modelo_nombre]['roc_auc']:.3f}")

print(f"\nüìã Ranking de Modelos (por Precisi√≥n en Test):")
# Usar la lista actualizada de nombres de modelos para el ranking
ranking = sorted(resultados.items(), key=lambda x: x[1]['accuracy'], reverse=True)
for i, (nombre, metricas) in enumerate(ranking, 1):
    print(f"   {i}. {nombre:<20}: {metricas['accuracy']:.2%}")

print(f"\nüí° Conclusi√≥n:")
print(f"   Este sistema de clasificaci√≥n alcanza una precisi√≥n m√°xima de {resultados[mejor_modelo_nombre]['accuracy']:.1%} con el modelo {mejor_modelo_nombre},")
print(f"   lo que lo hace altamente confiable. La optimizaci√≥n de hiperpar√°metros")
print(f"   permite ajustar el modelo ganador para obtener el mejor rendimiento posible.")
print(f"   La Curva de Aprendizaje tambi√©n ayuda a confirmar que el modelo no est√° sobreajustado.")

print(f"\n{'=' * 80}")
print("‚úÖ An√°lisis completo finalizado. Revisa los gr√°ficos generados.")
print(f"{'=' * 80}\n")
